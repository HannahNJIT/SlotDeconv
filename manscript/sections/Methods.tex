\section{Methods}
\subsection{Overview}
SlotDeconv is a three-stage framework for spatial transcriptomics (ST) deconvolution. Given scRNA-seq counts with cell-type annotations, ST counts, and spot coordinates, SlotDeconv (i) learns a discriminative cell-type signature matrix, (ii) obtains a convex warm start by weighted non-negative least squares (NNLS), and (iii) refines spot-wise proportions with a spatial neighborhood consistency prior.
Let $\mathbf{Y}\in\mathbb{R}_+^{C\times G_0}$ denote the scRNA-seq count matrix with $C$ cells and $G_0$ genes, and let $\boldsymbol{\tau}\in\{1,\ldots,K\}^C$ denote cell-type labels over $K$ cell types. Let $\mathbf{X}\in\mathbb{R}_+^{N\times G_0}$ denote the ST count matrix with $N$ spots, and let $\mathbf{C}\in\mathbb{R}^{N\times 2}$ denote spot coordinates. The output is the proportion matrix $\mathbf{V}\in\mathbb{R}_+^{N\times K}$, where each row lies on the simplex: $\sum_{k=1}^K V_{ik}=1$ and $V_{ik}\ge 0$.
Stage~1 learns a nonnegative, row-normalized signature matrix $\mathbf{B}\in\mathbb{R}_+^{K\times G}$ on a selected gene set of size $G$. Stage~2 computes an initialization $\mathbf{V}^{(\mathrm{init})}$ by weighted NNLS on library-size normalized ST expression. Stage~3 refines $\mathbf{V}$ by minimizing a KL reconstruction term plus a dense Gaussian-kernel spatial smoothness term.

\begin{table}[t]
\small
\caption{Notation summary.}
\label{tab:notation}
\centering
\begin{tabular}{cl}
\toprule
\textbf{Symbol} & \textbf{Description} \\
\midrule
$\mathbf{Y}\in\mathbb{R}_+^{C\times G_0}$ & scRNA-seq count matrix (cells $\times$ genes) \\
$\boldsymbol{\tau}\in\{1,\ldots,K\}^C$ & scRNA-seq cell-type labels \\
$\mathbf{X}\in\mathbb{R}_+^{N\times G_0}$ & ST count matrix (spots $\times$ genes) \\
$\mathbf{C}\in\mathbb{R}^{N\times 2}$ & spot coordinates \\
$\mathbf{S}\in\mathbb{R}^{K\times d}$ & learnable prototype (slot) vectors \\
$f_\theta$ & decoder network with parameters $\theta$ \\
$\tilde{\mathbf{b}}_k\in\mathbb{R}_+^{G}$ & unnormalized signature for cell type $k$ \\
$\mathbf{B}\in\mathbb{R}_+^{K\times G}$ & row-normalized signature matrix ($\sum_g B_{kg}=1$) \\
$\boldsymbol{\alpha}\in\mathbb{R}_+^{G}$ & gene-wise NB dispersion (shape) parameters \\
$\mathbf{w},\tilde{\mathbf{w}}\in\mathbb{R}_+^{G}$ & raw and transformed gene weights for NNLS \\
$\mathbf{V}\in\mathbb{R}_+^{N\times K}$ & cell-type proportions (rows on simplex) \\
$\mathbf{L}\in\mathbb{R}^{N\times K}$ & unconstrained logits for Stage~3 optimization \\
$\bar{\mathbf{A}}\in\mathbb{R}_+^{N\times N}$ & row-normalized dense spatial kernel with zero diagonal \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Data preprocessing}
\textbf{Cell balancing (scRNA-seq).} To reduce dominance of abundant cell types, we subsample each cell type to at most $M$ cells (default $M=750$), yielding a balanced scRNA-seq set.
\textbf{Gene selection and discriminative scores.} On the balanced scRNA-seq data, we compute gene-wise discriminative scores $\mathbf{w}\in\mathbb{R}_+^{G_0}$ using an ANOVA-like ratio of between-type to within-type variance. We select the top $G$ genes (default $G=3000$) and use the intersection of selected genes with genes present in the ST matrix for all subsequent stages.
\textbf{Median-normalized size factors.} For scRNA-seq cell $c$, define $s_c=\sum_g Y_{cg}$ and
\begin{equation}
\tilde{s}_c=\frac{s_c}{\mathrm{median}(\{s_{c'}:s_{c'}>0\})}.
\end{equation}
For ST spot $i$, define $l_i=\sum_g X_{ig}$ and
\begin{equation}
\tilde{l}_i=\frac{l_i}{\mathrm{median}(\{l_{i'}:l_{i'}>0\})}.
\end{equation}
These size factors are treated as \textbf{constants} in subsequent optimization.
\textbf{Transformed gene weights for NNLS.} To stabilize weighted NNLS, we apply power scaling and clipping (default $\gamma=0.8$):
\begin{equation}
\tilde{w}_g=\mathrm{clip}\!\left(\left(\frac{w_g}{\mathrm{median}(\mathbf{w})+\epsilon}\right)^{\gamma},\,0.2,\,5.0\right),
\end{equation}
and use $\sqrt{\tilde{w}_g}$ as diagonal weights in Stage~2.

\subsection{Model architecture and implementation}
\subsubsection{Stage 1: Constrained reference learning}
\textbf{Constants, parameters.} Constants are $\boldsymbol{\tau}$ and $\{\tilde{s}_c\}_{c=1}^C$. Trainable parameters are prototype vectors $\mathbf{S}\in\mathbb{R}^{K\times d}$ (default $d=128$), decoder parameters $\theta$, and gene-wise dispersion parameters $\boldsymbol{\alpha}\in\mathbb{R}_+^{G}$.
\textbf{Decoder architecture and forward computation.} The decoder is a 3-layer MLP with widths $d\!\to\!256\!\to\!512\!\to\!G$. It applies LayerNorm and ReLU after the first two linear layers, with a dropout rate of 0.1 applied after the first LayerNorm+ReLU block during training. For cell type $k$, we compute logits $\mathbf{h}_k=f_\theta(\mathbf{s}_k)$ and obtain a nonnegative unnormalized signature
\begin{equation}
\tilde{\mathbf{b}}_k=\mathrm{softplus}\!\big(\mathrm{clip}(\mathbf{h}_k,-20,20)\big)\in\mathbb{R}_+^{G}.
\end{equation}
We then define the row-normalized signature matrix used in Stage~2--3 and in the diversity term:
\begin{equation}
B_{kg}=\frac{\tilde{b}_{kg}}{\sum_{g'}\tilde{b}_{kg'}+\epsilon},\quad \mathbf{B}\in\mathbb{R}_+^{K\times G},\quad \sum_g B_{kg}=1.
\end{equation}
\textbf{Negative binomial likelihood.} For scRNA-seq cell $c$ with type $\tau(c)$, we model counts with mean $\mu_{cg}=\tilde{s}_c\cdot \tilde{b}_{\tau(c),g}$ and dispersion (shape) $\alpha_g$, using a mean--shape parameterization with $\mathrm{Var}(y_{cg})=\mu_{cg}+\mu_{cg}^2/\alpha_g$:
\begin{equation}
\mathcal{L}_{\mathrm{NB}}=-\frac{1}{CG}\sum_{c=1}^{C}\sum_{g=1}^{G}\log \mathrm{NB}(y_{cg};\mu_{cg},\alpha_g).
\end{equation}
\textbf{Worst-case max-margin diversity loss.} Let $\hat{\mathbf{B}}_k=\mathbf{B}_k/\|\mathbf{B}_k\|_2$ and $S_{ij}=\hat{\mathbf{B}}_i^\top \hat{\mathbf{B}}_j$. We penalize the most violating off-diagonal pair:
\begin{equation}
\mathcal{L}_{\mathrm{div}}=\max_{i\neq j}\big[ S_{ij}-m\big]_+,
\end{equation}
where $m$ is a margin (default $m=0.1$).
\textbf{Stage~1 objective.} We minimize
\begin{equation}
\mathcal{L}_1=\mathcal{L}_{\mathrm{NB}}+\lambda_{\mathrm{div}}\mathcal{L}_{\mathrm{div}},
\end{equation}
and optimize $\{\mathbf{S},\theta,\boldsymbol{\alpha}\}$ using AdamW with cosine annealing and gradient clipping (max norm 5.0). After convergence, we fix the learned $\mathbf{B}$ for Stage~2--3.

\subsubsection{Stage 2: Warm-start initialization (weighted NNLS)}
\textbf{Constants, variables.} Constants are $\mathbf{B}$, transformed gene weights $\tilde{\mathbf{w}}$, and ST size factors $\{\tilde{l}_i\}_{i=1}^{N}$. Variables are spot-wise proportions $\mathbf{v}_i^{(0)}\in\mathbb{R}_+^{K}$.
\textbf{Computation order.} We compute library-size normalized ST expression $\mathbf{x}_i^{(u)}=\mathbf{x}_i/\tilde{l}_i$ and solve weighted NNLS:
\begin{equation}
\mathbf{v}_i^{(0)}=\arg\min_{\mathbf{v}\ge 0}\left\|\mathrm{diag}(\tilde{\mathbf{w}}^{1/2})\left(\mathbf{x}_i^{(u)}-\mathbf{B}^\top\mathbf{v}\right)\right\|_2^2.
\end{equation}
We project onto the simplex by $\ell_1$ normalization:
\begin{equation}
\mathbf{v}_i^{(\mathrm{init})}=\frac{\mathbf{v}_i^{(0)}}{\sum_k v_{ik}^{(0)}}\ \text{if }\sum_k v_{ik}^{(0)}>0,\quad \text{else }\ \mathbf{v}_i^{(\mathrm{init})}=\mathbf{1}_K/K.
\end{equation}

\subsubsection{Stage 3: Spatial-aware refinement (logit parameterization)}
\textbf{Constants, variables.} Constants are $\mathbf{B}$ and the observed per-spot gene distributions $\mathbf{X}_{\mathrm{prob}}$. Variables are logits $\mathbf{L}\in\mathbb{R}^{N\times K}$ with row-wise softmax $\mathbf{V}=\mathrm{softmax}(\mathbf{L})$. Initialization is $\mathbf{L}^{(0)}=\log(\mathbf{V}^{(\mathrm{init})}+\epsilon)$.
\textbf{KL reconstruction term.} Define empirical gene distributions $X^{\mathrm{prob}}_{ig}=X_{ig}/(\sum_{g'}X_{ig'}+\epsilon)$ and predicted distributions from $\hat{\mathbf{x}}_i=\mathbf{v}_i\mathbf{B}$:
\begin{equation}
\hat{x}^{\mathrm{prob}}_{ig}=\frac{\hat{x}_{ig}}{\sum_{g'}\hat{x}_{ig'}+\epsilon}.
\end{equation}
We minimize $\mathrm{KL}(X_{\mathrm{prob}}\|\hat{X}_{\mathrm{prob}})$ up to an additive constant:
\begin{equation}
\mathcal{L}_{\mathrm{KL}}=\frac{1}{N}\sum_{i=1}^{N}\sum_{g=1}^{G}X^{\mathrm{prob}}_{ig}\left(\log(X^{\mathrm{prob}}_{ig}+\epsilon)-\log(\hat{x}^{\mathrm{prob}}_{ig}+\epsilon)\right).
\end{equation}
\textbf{Dense Gaussian-kernel neighborhood consistency.} We z-score normalize coordinates $\tilde{\mathbf{c}}_i=(\mathbf{c}_i-\bar{\mathbf{c}})/(\boldsymbol{\sigma}_{\mathbf{c}}+\epsilon)$ and compute pairwise distances $d_{ij}=\|\tilde{\mathbf{c}}_i-\tilde{\mathbf{c}}_j\|_2$. The bandwidth is set to $\sigma=\mathrm{median}(\{d_{ij}:d_{ij}>0\})$. We define a dense Gaussian kernel, remove self-loops, and row-normalize:
\begin{equation}
\tilde{A}_{ij}=\exp\!\left(-\frac{d_{ij}^2}{2\sigma^2}\right)(1-\delta_{ij}),\quad \bar{A}_{ij}=\frac{\tilde{A}_{ij}}{\sum_{j'}\tilde{A}_{ij'}+\epsilon}.
\end{equation}
The spatial smoothness loss is implemented as the mean squared deviation from neighborhood-averaged proportions:
\begin{equation}
\mathcal{L}_{\mathrm{sp}}=\frac{1}{NK}\sum_{i=1}^{N}\sum_{k=1}^{K}\left(V_{ik}-(\bar{\mathbf{A}}\mathbf{V})_{ik}\right)^2.
\end{equation}
\textbf{Stage~3 objective.} We minimize
\begin{equation}
\mathcal{L}_3=\mathcal{L}_{\mathrm{KL}}+\lambda_{\mathrm{sp}}\mathcal{L}_{\mathrm{sp}},
\end{equation}
and optimize $\mathbf{L}$ using Adam with cosine annealing.

\subsection{Implementation details}
SlotDeconv is implemented in Python using PyTorch and SciPy. Stage~2 NNLS is solved by \texttt{scipy.optimize.nnls}.
Default hyperparameters are $G=3000$, $M=750$, $d=128$, $\lambda_{\mathrm{div}}=4.0$, $m=0.1$, $\gamma=0.8$, and $\lambda_{\mathrm{sp}}=15.0$.
Stage~1 uses AdamW with learning rate $10^{-3}$, weight decay $10^{-5}$, and runs for 2000 epochs.
Stage~3 uses Adam with learning rate $10^{-2}$ and runs for 1500 epochs.
Both stages employ cosine annealing learning rate scheduling.