\section{Introduction}
Spatial transcriptomics (ST) enables gene expression profiling while preserving spatial context, but sequencing-based platforms such as 10x Visium and Slide-seq operate at spot-level resolution where each location aggregates transcripts from multiple cells~\cite{staahl2016visualization}. Cell-type deconvolution---inferring constituent cell-type proportions within each spot---has therefore become essential for downstream spatial analyses~\cite{gaspard2025cell}.


Existing deconvolution methods can be categorized along two dimensions: modeling approach (linear vs.\ nonlinear) and spatial awareness~\cite{gaspard2025cell,sang2024spotless}.
\textbf{Linear models without spatial information} dominate the field. Probabilistic methods include RCTD~\cite{cable2022robust} (likelihood-based with platform normalization), Cell2location~\cite{kleshchevnikov2022cell2location} (hierarchical Bayesian with negative binomial likelihood), Stereoscope~\cite{andersson2020single}, and DestVI~\cite{lopez2022destvi} (variational inference for continuous cell states). Matrix factorization approaches include SPOTlight~\cite{elosua2021spotlight} (NMF+NNLS) and SpatialDWLS~\cite{dong2021spatialdwls}. Optimal transport methods such as Tangram~\cite{biancalani2021deep} learn probabilistic cell-to-spot mappings.
\textbf{Linear models with spatial information} leverage tissue continuity. CARD~\cite{ma2022spatially} pioneered conditional autoregressive modeling to borrow information across neighbors. DeCoST~\cite{guo2025decost} extends this with Gaussian kernel-based CAR and domain adaptation for platform effects. SONAR~\cite{liu2023sonar} uses spatially weighted Poisson-Gamma models.
\textbf{Nonlinear deep learning approaches} capture complex relationships beyond linear mixtures. GraphST~\cite{long2023spatially} combines graph neural networks with contrastive learning for joint spatial clustering and deconvolution. Spotiphy~\cite{yang2025spotiphy} uses generative modeling to achieve pseudo-single-cell-resolution whole-transcriptome imaging.


Despite progress, existing methods face two fundamental challenges:
\textbf{(1) Reference signature quality.} Cluster-wise averaging from scRNA-seq produces highly collinear signatures for related subtypes (e.g., cortical layers L2/3, L4, L5), leading to ill-conditioned linear systems~\cite{cable2022robust,gaspard2025cell}.
\textbf{(2) Spatial smoothing trade-offs.} Methods incorporating spatial priors risk oversmoothing anatomical boundaries or provide insufficient denoising~\cite{ma2022spatially}.
To address these challenges, we propose SlotDeconv, occupying a middle ground between linear methods and end-to-end deep models: \textbf{nonlinear representation learning} for discriminative signatures combined with \textbf{linear mixture deconvolution} for interpretability.
Our three-stage framework:
\textbf{Stage 1: Constrained Reference Learning.} Each cell type is represented by a learnable prototype (``slot''), mapped to gene space through a neural decoder trained under negative binomial likelihood~\cite{love2014moderated}. A max-margin diversity loss penalizes excessive cosine similarity between signatures, improving discriminability for closely related subtypes. Unlike Slot Attention in object-centric vision, we do not employ iterative attention-based binding.
\textbf{Stage 2: Warm-Start Initialization.} Discriminative-gene-weighted NNLS (with gene-level F-statistics) provides a fast and stable initialization under a convex formulation.
\textbf{Stage 3: Spatial Refinement.} Starting from the NNLS solution, proportions are refined by minimizing KL divergence between empirical per-spot gene distributions and normalized reconstructions, together with a Gaussian-kernel neighborhood smoothness prior on the proportion matrix. The kernel bandwidth is set using the median inter-spot distance, reducing sensitivity to coordinate scaling.
\textbf{Contributions:} We (1) learn subtype-discriminative signatures via diversity-regularized prototype decoding under negative binomial modeling; (2) combine weighted NNLS warm-start with KL-based spatial refinement using a median-distance-scaled neighborhood kernel; and (3) achieve a 29\% relative improvement over NNLS on a mouse brain ST dataset with 27 cell types, with improved cortical-layer subtype resolution.
